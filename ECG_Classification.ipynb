{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_train = pd.read_csv('/scratch/gilbreth/lu992/final_50024_project/ecg_data/mitdb_360_train.csv', header=None)\n",
    "mit_test = pd.read_csv('/scratch/gilbreth/lu992/final_50024_project/ecg_data/mitdb_360_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from data\n",
    "y_train = mit_train[360]\n",
    "X_train = mit_train.loc[:, :359]\n",
    "\n",
    "y_test = mit_test[360]\n",
    "X_test = mit_test.loc[:, :359]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = map(\n",
    "    torch.from_numpy, \n",
    "    (X_train.values, y_train.values, X_test.values, y_test.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 3D tensor\n",
    "X_train = X_train.unsqueeze(1)\n",
    "X_test = X_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "bs = 128\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchdiffeq\n",
      "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: torch>=1.3.0 in /apps/gilbreth/ml/ml-toolkit/conda-2020.11-py38/gpu/install/pytorch-1.7.1/lib/python3.8/site-packages (from torchdiffeq) (1.7.1)\n",
      "Requirement already satisfied: scipy>=1.4.0 in /apps/gilbreth/ml/ml-toolkit/conda-2020.11-py38/gpu/install/theano-1.0.5/lib/python3.8/site-packages (from torchdiffeq) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /apps/gilbreth/ml/ml-toolkit/conda-2020.11-py38/gpu/install/theano-1.0.5/lib/python3.8/site-packages (from scipy>=1.4.0->torchdiffeq) (1.20.1)\n",
      "Requirement already satisfied: typing_extensions in /apps/gilbreth/ml/ml-toolkit/conda-2020.11-py38/gpu/install/tensorflow-2.4.0/lib/python3.8/site-packages (from torch>=1.3.0->torchdiffeq) (3.7.4.3)\n",
      "Installing collected packages: torchdiffeq\n",
      "Successfully installed torchdiffeq-0.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from models import norm, ResBlock, ODEfunc, ODENet, Flatten, count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers adapted from https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "\n",
    "def get_model(is_odenet=True, dim=64, adam=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Initialize ResNet or ODENet with optimizer.\n",
    "    \"\"\"\n",
    "    downsampling_layers = [\n",
    "        nn.Conv1d(1, dim, 3, 1),\n",
    "        norm(dim),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv1d(dim, dim, 4, 2, 1),\n",
    "        norm(dim),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv1d(dim, dim, 4, 2, 1)\n",
    "    ]\n",
    "\n",
    "    feature_layers = [ODENet(ODEfunc(dim), **kwargs)] if is_odenet else [ResBlock(dim) for _ in range(6)]\n",
    "\n",
    "    fc_layers = [norm(dim), nn.ReLU(inplace=True), nn.AdaptiveAvgPool1d(1), Flatten(), nn.Linear(dim, 5)]\n",
    "\n",
    "    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers)\n",
    "\n",
    "    opt = optim.Adam(model.parameters()) if adam else optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "    return model, opt\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    \"\"\"\n",
    "    Calculate loss and update weights if training. Return loss, number of examples, and number of correct predictions.\n",
    "    \"\"\"\n",
    "    output = model(xb.float())\n",
    "    loss = loss_func(output, yb.long())\n",
    "    preds = torch.argmax(output, dim=1)  # Get the index of the max log-probability\n",
    "    correct = (preds == yb).float().sum()  # Count correct predictions\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb), correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl, csv_filename='training_log.csv'):\n",
    "    \"\"\"\n",
    "    Train and evaluate the neural network model. Track and print training and validation loss and accuracy.\n",
    "    Log these metrics to a CSV file.\n",
    "    \"\"\"\n",
    "    # Open CSV file and set up CSV writer\n",
    "    with open(csv_filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header to CSV file\n",
    "        writer.writerow(['Epoch', 'Train Loss', 'Train Accuracy', 'Validation Loss', 'Validation Accuracy'])\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Training... epoch {epoch + 1}\")\n",
    "            \n",
    "            model.train()  # Set model to training mode\n",
    "            total_loss, total_correct, total = 0, 0, 0\n",
    "\n",
    "            batch_count = 0\n",
    "            start = time.time()\n",
    "            for xb, yb in train_dl:\n",
    "                loss, num, correct = loss_batch(model, loss_func, xb, yb, opt)\n",
    "                total_loss += loss * num\n",
    "                total_correct += correct\n",
    "                total += num\n",
    "                batch_count += 1\n",
    "                curr_time = time.time()\n",
    "                percent = round(batch_count/len(train_dl) * 100, 1)\n",
    "                elapsed = round((curr_time - start)/60, 1)\n",
    "                print(f\"    Percent trained: {percent}%  Time elapsed: {elapsed} min\", end='\\r')\n",
    "\n",
    "            train_loss = total_loss / total\n",
    "            train_acc = total_correct / total\n",
    "            \n",
    "            model.eval()  # Set model to validation mode\n",
    "            with torch.no_grad():\n",
    "                val_losses, val_nums, val_corrects = zip(\n",
    "                    *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "                )\n",
    "            \n",
    "            val_loss = sum(np.multiply(val_losses, val_nums)) / sum(val_nums)\n",
    "            val_acc = sum(val_corrects) / sum(val_nums)\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Valid Loss: {val_loss:.4f}, Valid Acc: {val_acc:.4f}\\n\")\n",
    "\n",
    "            # Write metrics to CSV file\n",
    "            writer.writerow([epoch + 1, train_loss, train_acc, val_loss, val_acc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "odenet, odeopt = get_model(adam=False, rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet, resopt = get_model(is_odenet=False, adam=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... epoch 1\n",
      "    Percent trained: 100.0%  Time elapsed: 30.8 min\n",
      "Epoch 1: Train Loss: 0.2960, Train Acc: 0.9145, Valid Loss: 0.9209, Valid Acc: 0.7537\n",
      "\n",
      "Training... epoch 2\n",
      "    Percent trained: 100.0%  Time elapsed: 37.1 min\n",
      "Epoch 2: Train Loss: 0.1434, Train Acc: 0.9614, Valid Loss: 0.6880, Valid Acc: 0.8094\n",
      "\n",
      "Training... epoch 3\n",
      "    Percent trained: 100.0%  Time elapsed: 38.6 min\n",
      "Epoch 3: Train Loss: 0.1041, Train Acc: 0.9708, Valid Loss: 0.6075, Valid Acc: 0.8343\n",
      "\n",
      "Training... epoch 4\n",
      "    Percent trained: 100.0%  Time elapsed: 42.4 min\n",
      "Epoch 4: Train Loss: 0.0854, Train Acc: 0.9760, Valid Loss: 0.6123, Valid Acc: 0.8086\n",
      "\n",
      "Training... epoch 5\n",
      "    Percent trained: 100.0%  Time elapsed: 45.2 min\n",
      "Epoch 5: Train Loss: 0.0715, Train Acc: 0.9800, Valid Loss: 0.4547, Valid Acc: 0.8691\n",
      "\n",
      "Training... epoch 6\n",
      "    Percent trained: 100.0%  Time elapsed: 47.0 min\n",
      "Epoch 6: Train Loss: 0.0643, Train Acc: 0.9814, Valid Loss: 0.3805, Valid Acc: 0.8854\n",
      "\n",
      "Training... epoch 7\n",
      "    Percent trained: 100.0%  Time elapsed: 48.1 min\n",
      "Epoch 7: Train Loss: 0.0572, Train Acc: 0.9837, Valid Loss: 0.2844, Valid Acc: 0.9240\n",
      "\n",
      "Training... epoch 8\n",
      "    Percent trained: 100.0%  Time elapsed: 50.5 min\n",
      "Epoch 8: Train Loss: 0.0535, Train Acc: 0.9847, Valid Loss: 0.3157, Valid Acc: 0.9166\n",
      "\n",
      "Training... epoch 9\n",
      "    Percent trained: 100.0%  Time elapsed: 52.2 min\n",
      "Epoch 9: Train Loss: 0.0500, Train Acc: 0.9856, Valid Loss: 0.2593, Valid Acc: 0.9271\n",
      "\n",
      "Training... epoch 10\n",
      "    Percent trained: 100.0%  Time elapsed: 52.6 min\n",
      "Epoch 10: Train Loss: 0.0461, Train Acc: 0.9867, Valid Loss: 0.2046, Valid Acc: 0.9411\n",
      "\n",
      "Training... epoch 11\n",
      "    Percent trained: 100.0%  Time elapsed: 53.1 min\n",
      "Epoch 11: Train Loss: 0.0427, Train Acc: 0.9878, Valid Loss: 0.2033, Valid Acc: 0.9400\n",
      "\n",
      "Training... epoch 12\n",
      "    Percent trained: 100.0%  Time elapsed: 53.1 min\n",
      "Epoch 12: Train Loss: 0.0399, Train Acc: 0.9884, Valid Loss: 0.3280, Valid Acc: 0.9020\n",
      "\n",
      "Training... epoch 13\n",
      "    Percent trained: 100.0%  Time elapsed: 53.0 min\n",
      "Epoch 13: Train Loss: 0.0378, Train Acc: 0.9888, Valid Loss: 0.2027, Valid Acc: 0.9391\n",
      "\n",
      "Training... epoch 14\n",
      "    Percent trained: 100.0%  Time elapsed: 52.9 min\n",
      "Epoch 14: Train Loss: 0.0359, Train Acc: 0.9892, Valid Loss: 0.1938, Valid Acc: 0.9374\n",
      "\n",
      "Training... epoch 15\n",
      "    Percent trained: 100.0%  Time elapsed: 53.0 min\n",
      "Epoch 15: Train Loss: 0.0345, Train Acc: 0.9897, Valid Loss: 0.1731, Valid Acc: 0.9414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(15, odenet, F.cross_entropy, odeopt, train_dl, test_dl, csv_filename='odenet_training_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... epoch 1\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 1: Train Loss: 0.3096, Train Acc: 0.9123, Valid Loss: 1.0560, Valid Acc: 0.7277\n",
      "\n",
      "Training... epoch 2\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 2: Train Loss: 0.1383, Train Acc: 0.9622, Valid Loss: 0.6655, Valid Acc: 0.7783\n",
      "\n",
      "Training... epoch 3\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 3: Train Loss: 0.0971, Train Acc: 0.9729, Valid Loss: 0.3477, Valid Acc: 0.8946\n",
      "\n",
      "Training... epoch 4\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 4: Train Loss: 0.0779, Train Acc: 0.9786, Valid Loss: 0.2969, Valid Acc: 0.9043\n",
      "\n",
      "Training... epoch 5\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 5: Train Loss: 0.0650, Train Acc: 0.9817, Valid Loss: 0.2652, Valid Acc: 0.9263\n",
      "\n",
      "Training... epoch 6\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 6: Train Loss: 0.0547, Train Acc: 0.9844, Valid Loss: 0.2282, Valid Acc: 0.9334\n",
      "\n",
      "Training... epoch 7\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 7: Train Loss: 0.0488, Train Acc: 0.9858, Valid Loss: 0.1901, Valid Acc: 0.9434\n",
      "\n",
      "Training... epoch 8\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 8: Train Loss: 0.0441, Train Acc: 0.9870, Valid Loss: 0.2190, Valid Acc: 0.9283\n",
      "\n",
      "Training... epoch 9\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 9: Train Loss: 0.0392, Train Acc: 0.9883, Valid Loss: 0.2226, Valid Acc: 0.9309\n",
      "\n",
      "Training... epoch 10\n",
      "    Percent trained: 100.0%  Time elapsed: 3.0 min\n",
      "Epoch 10: Train Loss: 0.0371, Train Acc: 0.9890, Valid Loss: 0.1671, Valid Acc: 0.9474\n",
      "\n",
      "Training... epoch 11\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 11: Train Loss: 0.0349, Train Acc: 0.9895, Valid Loss: 0.2149, Valid Acc: 0.9317\n",
      "\n",
      "Training... epoch 12\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 12: Train Loss: 0.0312, Train Acc: 0.9903, Valid Loss: 0.1792, Valid Acc: 0.9437\n",
      "\n",
      "Training... epoch 13\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 13: Train Loss: 0.0303, Train Acc: 0.9907, Valid Loss: 0.2626, Valid Acc: 0.9189\n",
      "\n",
      "Training... epoch 14\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 14: Train Loss: 0.0268, Train Acc: 0.9919, Valid Loss: 0.1324, Valid Acc: 0.9591\n",
      "\n",
      "Training... epoch 15\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 15: Train Loss: 0.0269, Train Acc: 0.9916, Valid Loss: 0.1242, Valid Acc: 0.9571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(15, resnet, F.cross_entropy, resopt, train_dl, test_dl, csv_filename='resnet_training_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test.float())\n",
    "    preds = torch.argmax(F.softmax(logits, dim=1), axis=1).numpy()\n",
    "    return (preds == y_test.numpy()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing accuracy after 10 epochs for ResNet and ODENet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet accuracy: 0.957\n",
      "ODENet accuracy: 0.941\n"
     ]
    }
   ],
   "source": [
    "print(f\"ResNet accuracy: {round(accuracy(resnet, X_test, y_test), 3)}\")\n",
    "print(f\"ODENet accuracy: {round(accuracy(odenet, X_test, y_test), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tunable parameters in...\n",
      "    ResNet: 182853\n",
      "    ODENet: 59333\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tunable parameters in...\")\n",
    "print(f\"    ResNet: {count_parameters(resnet)}\")\n",
    "print(f\"    ODENet: {count_parameters(odenet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Learning [learning/conda-2020.11-py38-gpu]",
   "language": "python",
   "name": "sys_learning38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
