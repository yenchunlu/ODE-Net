{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_train = pd.read_csv('/scratch/gilbreth/lu992/final_50024_project/ecg_data/mitdb_360_train.csv', header=None)\n",
    "mit_test = pd.read_csv('/scratch/gilbreth/lu992/final_50024_project/ecg_data/mitdb_360_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from data\n",
    "y_train = mit_train[360]\n",
    "X_train = mit_train.loc[:, :359]\n",
    "\n",
    "y_test = mit_test[360]\n",
    "X_test = mit_test.loc[:, :359]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = map(\n",
    "    torch.from_numpy, \n",
    "    (X_train.values, y_train.values, X_test.values, y_test.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 3D tensor\n",
    "X_train = X_train.unsqueeze(1)\n",
    "X_test = X_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "bs = 128\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchdiffeq\n",
      "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: torch>=1.3.0 in /apps/gilbreth/ml/ml-toolkit/conda-2020.11-py38/gpu/install/pytorch-1.7.1/lib/python3.8/site-packages (from torchdiffeq) (1.7.1)\n",
      "Requirement already satisfied: scipy>=1.4.0 in /apps/gilbreth/ml/ml-toolkit/conda-2020.11-py38/gpu/install/theano-1.0.5/lib/python3.8/site-packages (from torchdiffeq) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /apps/gilbreth/ml/ml-toolkit/conda-2020.11-py38/gpu/install/theano-1.0.5/lib/python3.8/site-packages (from scipy>=1.4.0->torchdiffeq) (1.20.1)\n",
      "Requirement already satisfied: typing_extensions in /apps/gilbreth/ml/ml-toolkit/conda-2020.11-py38/gpu/install/tensorflow-2.4.0/lib/python3.8/site-packages (from torch>=1.3.0->torchdiffeq) (3.7.4.3)\n",
      "Installing collected packages: torchdiffeq\n",
      "Successfully installed torchdiffeq-0.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from models import norm, ResBlock, ODEfunc, ODENet, Flatten, count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers adapted from https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "\n",
    "def get_model(is_odenet=True, dim=64, adam=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Initialize ResNet or ODENet with optimizer.\n",
    "    \"\"\"\n",
    "    downsampling_layers = [\n",
    "        nn.Conv1d(1, dim, 3, 1),\n",
    "        norm(dim),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv1d(dim, dim, 4, 2, 1),\n",
    "        norm(dim),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv1d(dim, dim, 4, 2, 1)\n",
    "    ]\n",
    "\n",
    "    feature_layers = [ODENet(ODEfunc(dim), **kwargs)] if is_odenet else [ResBlock(dim) for _ in range(6)]\n",
    "\n",
    "    fc_layers = [norm(dim), nn.ReLU(inplace=True), nn.AdaptiveAvgPool1d(1), Flatten(), nn.Linear(dim, 5)]\n",
    "\n",
    "    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers)\n",
    "\n",
    "    opt = optim.Adam(model.parameters()) if adam else optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "    return model, opt\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    \"\"\"\n",
    "    Calculate loss and update weights if training. Return loss, number of examples, and number of correct predictions.\n",
    "    \"\"\"\n",
    "    output = model(xb.float())\n",
    "    loss = loss_func(output, yb.long())\n",
    "    preds = torch.argmax(output, dim=1)  # Get the index of the max log-probability\n",
    "    correct = (preds == yb).float().sum()  # Count correct predictions\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb), correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl, csv_filename='training_log.csv'):\n",
    "    \"\"\"\n",
    "    Train and evaluate the neural network model. Track and print training and validation loss and accuracy.\n",
    "    Log these metrics to a CSV file.\n",
    "    \"\"\"\n",
    "    # Open CSV file and set up CSV writer\n",
    "    with open(csv_filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header to CSV file\n",
    "        writer.writerow(['Epoch', 'Train Loss', 'Train Accuracy', 'Validation Loss', 'Validation Accuracy'])\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Training... epoch {epoch + 1}\")\n",
    "            \n",
    "            model.train()  # Set model to training mode\n",
    "            total_loss, total_correct, total = 0, 0, 0\n",
    "\n",
    "            batch_count = 0\n",
    "            start = time.time()\n",
    "            for xb, yb in train_dl:\n",
    "                loss, num, correct = loss_batch(model, loss_func, xb, yb, opt)\n",
    "                total_loss += loss * num\n",
    "                total_correct += correct\n",
    "                total += num\n",
    "                batch_count += 1\n",
    "                curr_time = time.time()\n",
    "                percent = round(batch_count/len(train_dl) * 100, 1)\n",
    "                elapsed = round((curr_time - start)/60, 1)\n",
    "                print(f\"    Percent trained: {percent}%  Time elapsed: {elapsed} min\", end='\\r')\n",
    "\n",
    "            train_loss = total_loss / total\n",
    "            train_acc = total_correct / total\n",
    "            \n",
    "            model.eval()  # Set model to validation mode\n",
    "            with torch.no_grad():\n",
    "                val_losses, val_nums, val_corrects = zip(\n",
    "                    *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "                )\n",
    "            \n",
    "            val_loss = sum(np.multiply(val_losses, val_nums)) / sum(val_nums)\n",
    "            val_acc = sum(val_corrects) / sum(val_nums)\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Valid Loss: {val_loss:.4f}, Valid Acc: {val_acc:.4f}\\n\")\n",
    "\n",
    "            # Write metrics to CSV file\n",
    "            writer.writerow([epoch + 1, train_loss, train_acc, val_loss, val_acc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "odenet, odeopt = get_model(adam=False, rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet, resopt = get_model(is_odenet=False, adam=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... epoch 1\n",
      "    Percent trained: 100.0%  Time elapsed: 29.9 min\n",
      "Epoch 1: Train Loss: 0.2985, Train Acc: 0.9158, Valid Loss: 1.0430, Valid Acc: 0.7011\n",
      "\n",
      "Training... epoch 2\n",
      "    Percent trained: 100.0%  Time elapsed: 29.7 min\n",
      "Epoch 2: Train Loss: 0.1379, Train Acc: 0.9625, Valid Loss: 0.9565, Valid Acc: 0.7300\n",
      "\n",
      "Training... epoch 3\n",
      "    Percent trained: 100.0%  Time elapsed: 33.2 min\n",
      "Epoch 3: Train Loss: 0.1014, Train Acc: 0.9718, Valid Loss: 0.4940, Valid Acc: 0.8360\n",
      "\n",
      "Training... epoch 4\n",
      "    Percent trained: 100.0%  Time elapsed: 37.4 min\n",
      "Epoch 4: Train Loss: 0.0824, Train Acc: 0.9774, Valid Loss: 0.4127, Valid Acc: 0.8680\n",
      "\n",
      "Training... epoch 5\n",
      "    Percent trained: 100.0%  Time elapsed: 37.9 min\n",
      "Epoch 5: Train Loss: 0.0734, Train Acc: 0.9793, Valid Loss: 0.3401, Valid Acc: 0.8963\n",
      "\n",
      "Training... epoch 6\n",
      "    Percent trained: 100.0%  Time elapsed: 38.6 min\n",
      "Epoch 6: Train Loss: 0.0654, Train Acc: 0.9813, Valid Loss: 0.6042, Valid Acc: 0.8226\n",
      "\n",
      "Training... epoch 7\n",
      "    Percent trained: 100.0%  Time elapsed: 42.6 min\n",
      "Epoch 7: Train Loss: 0.0586, Train Acc: 0.9833, Valid Loss: 0.3381, Valid Acc: 0.8926\n",
      "\n",
      "Training... epoch 8\n",
      "    Percent trained: 100.0%  Time elapsed: 44.5 min\n",
      "Epoch 8: Train Loss: 0.0541, Train Acc: 0.9844, Valid Loss: 0.2083, Valid Acc: 0.9366\n",
      "\n",
      "Training... epoch 9\n",
      "    Percent trained: 100.0%  Time elapsed: 45.2 min\n",
      "Epoch 9: Train Loss: 0.0503, Train Acc: 0.9855, Valid Loss: 0.1779, Valid Acc: 0.9423\n",
      "\n",
      "Training... epoch 10\n",
      "    Percent trained: 100.0%  Time elapsed: 47.6 min\n",
      "Epoch 10: Train Loss: 0.0443, Train Acc: 0.9873, Valid Loss: 0.2838, Valid Acc: 0.9146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(10, odenet, F.cross_entropy, odeopt, train_dl, test_dl, csv_filename='odenet_training_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... epoch 1\n",
      "    Percent trained: 100.0%  Time elapsed: 2.8 min\n",
      "Epoch 1: Train Loss: 0.3164, Train Acc: 0.9061, Valid Loss: 1.4931, Valid Acc: 0.5889\n",
      "\n",
      "Training... epoch 2\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 2: Train Loss: 0.1478, Train Acc: 0.9599, Valid Loss: 0.8591, Valid Acc: 0.7534\n",
      "\n",
      "Training... epoch 3\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 3: Train Loss: 0.1062, Train Acc: 0.9698, Valid Loss: 0.4578, Valid Acc: 0.8397\n",
      "\n",
      "Training... epoch 4\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 4: Train Loss: 0.0785, Train Acc: 0.9778, Valid Loss: 0.6815, Valid Acc: 0.8011\n",
      "\n",
      "Training... epoch 5\n",
      "    Percent trained: 100.0%  Time elapsed: 2.8 min\n",
      "Epoch 5: Train Loss: 0.0689, Train Acc: 0.9806, Valid Loss: 0.5539, Valid Acc: 0.8546\n",
      "\n",
      "Training... epoch 6\n",
      "    Percent trained: 100.0%  Time elapsed: 2.9 min\n",
      "Epoch 6: Train Loss: 0.0606, Train Acc: 0.9829, Valid Loss: 0.3755, Valid Acc: 0.8963\n",
      "\n",
      "Training... epoch 7\n",
      "    Percent trained: 100.0%  Time elapsed: 3.0 min\n",
      "Epoch 7: Train Loss: 0.0554, Train Acc: 0.9839, Valid Loss: 0.2787, Valid Acc: 0.9214\n",
      "\n",
      "Training... epoch 8\n",
      "    Percent trained: 100.0%  Time elapsed: 2.8 min\n",
      "Epoch 8: Train Loss: 0.0494, Train Acc: 0.9858, Valid Loss: 0.2092, Valid Acc: 0.9369\n",
      "\n",
      "Training... epoch 9\n",
      "    Percent trained: 100.0%  Time elapsed: 2.8 min\n",
      "Epoch 9: Train Loss: 0.0433, Train Acc: 0.9874, Valid Loss: 0.2003, Valid Acc: 0.9394\n",
      "\n",
      "Training... epoch 10\n",
      "    Percent trained: 100.0%  Time elapsed: 2.8 min\n",
      "Epoch 10: Train Loss: 0.0428, Train Acc: 0.9874, Valid Loss: 0.2255, Valid Acc: 0.9323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(10, resnet, F.cross_entropy, resopt, train_dl, test_dl, csv_filename='resnet_training_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test.float())\n",
    "    preds = torch.argmax(F.softmax(logits, dim=1), axis=1).numpy()\n",
    "    return (preds == y_test.numpy()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing accuracy after 10 epochs for ResNet and ODENet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet accuracy: 0.932\n",
      "ODENet accuracy: 0.915\n"
     ]
    }
   ],
   "source": [
    "print(f\"ResNet accuracy: {round(accuracy(resnet, X_test, y_test), 3)}\")\n",
    "print(f\"ODENet accuracy: {round(accuracy(odenet, X_test, y_test), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tunable parameters in...\n",
      "    ResNet: 182853\n",
      "    ODENet: 59333\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tunable parameters in...\")\n",
    "print(f\"    ResNet: {count_parameters(resnet)}\")\n",
    "print(f\"    ODENet: {count_parameters(odenet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Learning [learning/conda-2020.11-py38-gpu]",
   "language": "python",
   "name": "sys_learning38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
